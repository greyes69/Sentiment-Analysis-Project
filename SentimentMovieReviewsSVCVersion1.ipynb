{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyes69/Sentiment-Analysis-Project/blob/main/SentimentMovieReviewsSVCVersion1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "fe11cbad-ce4c-4cb3-b357-0c13ff9ffe61",
      "metadata": {
        "id": "fe11cbad-ce4c-4cb3-b357-0c13ff9ffe61",
        "outputId": "23f51cde-ccca-4536-9f52-7d1f884c9a4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "#from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n",
        "import joblib\n",
        "import re\n",
        "import nltk\n",
        "import ssl\n",
        "\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "V5rzDLC_Vo12",
        "outputId": "71a32d67-515a-435c-9b7c-baf8f490866c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "V5rzDLC_Vo12",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access files in your Google Drive\n",
        "df = pd.read_csv('/content/drive/MyDrive/IMDB Dataset.csv')\n",
        "\n",
        "# Assuming you have your dataset in a pandas DataFrame with two columns: 'reviews' and 'label'\n",
        "#df = pd.read_csv('IMDB Dataset.csv')  # Uncomment and load your dataset accordingly\n"
      ],
      "metadata": {
        "id": "rCcoZw8bVqlb"
      },
      "id": "rCcoZw8bVqlb",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "3468752d-7c77-41d8-9a46-bb30773d84f1",
      "metadata": {
        "id": "3468752d-7c77-41d8-9a46-bb30773d84f1"
      },
      "outputs": [],
      "source": [
        "df = df.rename(columns={'review': 'reviews'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0d7ad157-3dce-4acd-ac9a-508187cb8887",
      "metadata": {
        "id": "0d7ad157-3dce-4acd-ac9a-508187cb8887"
      },
      "outputs": [],
      "source": [
        "# Custom list of very frequent words (you can update this list based on your dataset)\n",
        "custom_stop_words = set(['movie', 'film', 'character', 'plot'])  # Add domain-specific frequent words if needed\n",
        "all_stop_words = list(ENGLISH_STOP_WORDS.union(custom_stop_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "079cebaf-4862-47b1-8213-afeb23e71e79",
      "metadata": {
        "id": "079cebaf-4862-47b1-8213-afeb23e71e79"
      },
      "outputs": [],
      "source": [
        "# Function to preprocess text: remove symbols, stop words, frequent words, etc.\n",
        "def preprocess_text(text):\n",
        "    # 1. Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Remove punctuation and special characters using regex\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Only keep alphanumeric and spaces\n",
        "\n",
        "    # 3. Tokenize the text into words\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # 4. Remove stopwords (optional, if you want to keep stopwords, skip this step)\n",
        "    #stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in all_stop_words]\n",
        "\n",
        "    # 5. Apply stemming to each word\n",
        "    #stemmed_tokens = [PorterStemmer.stem(token) for token in tokens]\n",
        "    p_stemmer = PorterStemmer()\n",
        "\n",
        "    # stem tokenized text and print first 500 tokens\n",
        "    stemmed_tokens = [p_stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    return ' '.join(stemmed_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "5c31ee81-77df-4892-9277-93ca4a64c288",
      "metadata": {
        "id": "5c31ee81-77df-4892-9277-93ca4a64c288"
      },
      "outputs": [],
      "source": [
        "# Preprocess the reviews\n",
        "df['clean_reviews'] = df['reviews'].apply(preprocess_text)\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['clean_reviews'], df['sentiment'], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "5216255b-fe69-4300-8f5d-b6124c311fc6",
      "metadata": {
        "id": "5216255b-fe69-4300-8f5d-b6124c311fc6"
      },
      "outputs": [],
      "source": [
        "# Create a TfidfVectorizer to extract 1-grams and 2-grams, remove stop words, and limit frequent words\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_df=0.85, min_df=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "508edb3c-8eef-4812-a1cb-7d01a93c8196",
      "metadata": {
        "id": "508edb3c-8eef-4812-a1cb-7d01a93c8196"
      },
      "outputs": [],
      "source": [
        "# Define the  model pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', tfidf_vectorizer),  # Use TF-IDF instead of raw counts\n",
        "    ('svc', LinearSVC()) # Use linear kernel for classification\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "0048cbbe-224f-416f-ad72-7e0392d39feb",
      "metadata": {
        "id": "0048cbbe-224f-416f-ad72-7e0392d39feb",
        "outputId": "f3fdd3fc-1d09-4794-bbb3-8a1e079fb042",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "Best Hyperparameters: {'svc__C': 0.5, 'svc__max_iter': 500000, 'svc__tol': 0.001}\n",
            "Best Accuracy Score: 0.897875\n",
            "Test Set Accuracy: 0.9052\n"
          ]
        }
      ],
      "source": [
        "# Define the hyperparameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'svc__C': [0.5,1,5],  # Regularization parameter\n",
        "    'svc__tol': [1e-2, 1e-3],  # Tolerance for stopping criteria\n",
        "    'svc__max_iter': [500000, 1000000],  # Maximum iterations\n",
        "}\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, verbose = 2, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Extract the best pipeline, including the fitted TfidfVectorizer\n",
        "best_pipeline = grid_search.best_estimator_\n",
        "\n",
        "# Print the best parameters and the best score\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best Accuracy Score:\", grid_search.best_score_)\n",
        "\n",
        "# Save the entire pipeline (including the TfidfVectorizer) to a file\n",
        "#joblib.dump(best_pipeline, 'best_pipeline.pkl')\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "test_score = best_pipeline.score(X_test, y_test)\n",
        "print(\"Test Set Accuracy:\", test_score)\n",
        "\n",
        "# Access the fitted TfidfVectorizer\n",
        "fitted_tfidf = best_pipeline.named_steps['tfidf']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ae408cca-deb7-474e-8078-34e7310f1554",
      "metadata": {
        "id": "ae408cca-deb7-474e-8078-34e7310f1554",
        "outputId": "3fcaae4f-9153-4b5b-9a1b-cf8f61cab59f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your review:I have seen better movies\n"
          ]
        }
      ],
      "source": [
        "Xnew = input('Enter your review:')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "d78ab0ca-a7d5-42c7-9431-bb6492a7fb2e",
      "metadata": {
        "id": "d78ab0ca-a7d5-42c7-9431-bb6492a7fb2e"
      },
      "outputs": [],
      "source": [
        "# Preprocessing and tfidf-ing the new review\n",
        "df1= pd.DataFrame.from_dict({'Newreview':[Xnew]})\n",
        "FeaturesXnew = df1['Newreview'].apply(preprocess_text)\n",
        "FeaturesXnewtfidf = fitted_tfidf.transform(FeaturesXnew)\n",
        "fitted_svc = best_pipeline.named_steps['svc']\n",
        "ynew=fitted_svc.predict(FeaturesXnewtfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "38d4e3e9-49b3-42dd-b4ec-1a190eb37871",
      "metadata": {
        "id": "38d4e3e9-49b3-42dd-b4ec-1a190eb37871",
        "outputId": "bafc13a5-9931-439f-89c9-cb5f945f43cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I have seen better movies - This review is negative\n"
          ]
        }
      ],
      "source": [
        "print(\"%s - This review is %s\" % (Xnew, ynew[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "408b2f66-fdbe-4f63-bb42-2962e3c4d7f6",
      "metadata": {
        "id": "408b2f66-fdbe-4f63-bb42-2962e3c4d7f6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}