{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe11cbad-ce4c-4cb3-b357-0c13ff9ffe61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/reyessou/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import joblib\n",
    "import re\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91449994-a56f-4990-a0ef-487314e0325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have your dataset in a pandas DataFrame with two columns: 'reviews' and 'label'\n",
    "df = pd.read_csv('IMDB Dataset.csv')  # Uncomment and load your dataset accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3468752d-7c77-41d8-9a46-bb30773d84f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'review': 'reviews'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d7ad157-3dce-4acd-ac9a-508187cb8887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom list of very frequent words (you can update this list based on your dataset)\n",
    "custom_stop_words = set(['movie', 'film', 'character', 'plot'])  # Add domain-specific frequent words if needed\n",
    "all_stop_words = list(ENGLISH_STOP_WORDS.union(custom_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "079cebaf-4862-47b1-8213-afeb23e71e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text: remove symbols, stop words, frequent words, etc.\n",
    "def preprocess_text(text):\n",
    "    # 1. Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Remove punctuation and special characters using regex\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Only keep alphanumeric and spaces\n",
    "    \n",
    "    # 3. Tokenize the text into words\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 4. Remove stopwords (optional, if you want to keep stopwords, skip this step)\n",
    "    #stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in all_stop_words]\n",
    "    \n",
    "    # 5. Apply stemming to each word\n",
    "    #stemmed_tokens = [PorterStemmer.stem(token) for token in tokens]\n",
    "    p_stemmer = PorterStemmer()\n",
    "\n",
    "    # stem tokenized text and print first 500 tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c31ee81-77df-4892-9277-93ca4a64c288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the reviews\n",
    "df['clean_reviews'] = df['reviews'].apply(preprocess_text)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['clean_reviews'], df['sentiment'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5216255b-fe69-4300-8f5d-b6124c311fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TfidfVectorizer to extract 1-grams and 2-grams, remove stop words, and limit frequent words\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_df=0.85, min_df=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "508edb3c-8eef-4812-a1cb-7d01a93c8196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the  model pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', tfidf_vectorizer),  # Use TF-IDF instead of raw counts\n",
    "    ('svc', LinearSVC()) # Use linear kernel for classification\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0048cbbe-224f-416f-ad72-7e0392d39feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best Hyperparameters: {'svc__C': 1, 'svc__max_iter': 500000, 'svc__tol': 0.01}\n",
      "Best Accuracy Score: 0.89645\n",
      "Test Set Accuracy: 0.9037\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'svc__C': [0.5,1,5],  # Regularization parameter\n",
    "    'svc__tol': [1e-2, 1e-3],  # Tolerance for stopping criteria\n",
    "    'svc__max_iter': [500000, 1000000],  # Maximum iterations\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, verbose = 2, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract the best pipeline, including the fitted TfidfVectorizer\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy Score:\", grid_search.best_score_)\n",
    "\n",
    "# Save the entire pipeline (including the TfidfVectorizer) to a file\n",
    "#joblib.dump(best_pipeline, 'best_pipeline.pkl')\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "test_score = best_pipeline.score(X_test, y_test)\n",
    "print(\"Test Set Accuracy:\", test_score)\n",
    "\n",
    "# Access the fitted TfidfVectorizer\n",
    "fitted_tfidf = best_pipeline.named_steps['tfidf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae408cca-deb7-474e-8078-34e7310f1554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your review: The movie was not different from many others, it did not leave an impression\n"
     ]
    }
   ],
   "source": [
    "Xnew = input('Enter your review:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d78ab0ca-a7d5-42c7-9431-bb6492a7fb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing and tfidf-ing the new review\n",
    "df1= pd.DataFrame.from_dict({'Newreview':[Xnew]})\n",
    "FeaturesXnew = df1['Newreview'].apply(preprocess_text)\n",
    "FeaturesXnewtfidf = fitted_tfidf.transform(FeaturesXnew)\n",
    "fitted_svc = best_pipeline.named_steps['svc']\n",
    "ynew=fitted_svc.predict(FeaturesXnewtfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38d4e3e9-49b3-42dd-b4ec-1a190eb37871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie was not different from many others, it did not leave an impression - This review is negative\n",
      "[CV] END .......svc__C=1, svc__max_iter=100000, svc__tol=0.1; total time=  24.4s\n",
      "[CV] END ......svc__C=1, svc__max_iter=500000, svc__tol=0.01; total time=  16.1s\n",
      "[CV] END .......svc__C=5, svc__max_iter=100000, svc__tol=0.1; total time=  18.0s\n",
      "[CV] END .......svc__C=5, svc__max_iter=500000, svc__tol=0.1; total time=  18.6s\n",
      "[CV] END ......svc__C=10, svc__max_iter=100000, svc__tol=0.1; total time=  19.0s\n",
      "[CV] END .......svc__C=1, svc__max_iter=100000, svc__tol=0.1; total time=  24.9s\n",
      "[CV] END .......svc__C=1, svc__max_iter=500000, svc__tol=0.1; total time=  14.7s\n",
      "[CV] END .......svc__C=5, svc__max_iter=100000, svc__tol=0.1; total time=  18.6s\n",
      "[CV] END .......svc__C=5, svc__max_iter=500000, svc__tol=0.1; total time=  19.0s\n",
      "[CV] END ......svc__C=10, svc__max_iter=100000, svc__tol=0.1; total time=  19.5s\n",
      "[CV] END ......svc__C=1, svc__max_iter=100000, svc__tol=0.01; total time=  24.4s\n",
      "[CV] END ......svc__C=1, svc__max_iter=500000, svc__tol=0.01; total time=  15.8s\n",
      "[CV] END .......svc__C=5, svc__max_iter=100000, svc__tol=0.1; total time=  18.0s\n",
      "[CV] END .......svc__C=5, svc__max_iter=500000, svc__tol=0.1; total time=  20.0s\n",
      "[CV] END .....svc__C=10, svc__max_iter=500000, svc__tol=0.01; total time=  20.7s\n",
      "[CV] END .......svc__C=1, svc__max_iter=100000, svc__tol=0.1; total time=  23.0s\n",
      "[CV] END ......svc__C=1, svc__max_iter=500000, svc__tol=0.01; total time=  15.9s\n",
      "[CV] END ......svc__C=5, svc__max_iter=100000, svc__tol=0.01; total time=  19.1s\n",
      "[CV] END .......svc__C=5, svc__max_iter=500000, svc__tol=0.1; total time=  19.3s\n",
      "[CV] END .....svc__C=10, svc__max_iter=500000, svc__tol=0.01; total time=  22.3s\n",
      "[CV] END ......svc__C=1, svc__max_iter=100000, svc__tol=0.01; total time=  27.8s\n",
      "[CV] END .......svc__C=1, svc__max_iter=500000, svc__tol=0.1; total time=  16.0s\n",
      "[CV] END .......svc__C=5, svc__max_iter=100000, svc__tol=0.1; total time=  17.4s\n",
      "[CV] END .....svc__C=10, svc__max_iter=100000, svc__tol=0.01; total time=  20.4s\n",
      "[CV] END .....svc__C=10, svc__max_iter=500000, svc__tol=0.01; total time=  19.7s\n",
      "[CV] END ......svc__C=1, svc__max_iter=100000, svc__tol=0.01; total time=  28.5s\n",
      "[CV] END ......svc__C=5, svc__max_iter=100000, svc__tol=0.01; total time=  17.8s\n",
      "[CV] END .......svc__C=5, svc__max_iter=500000, svc__tol=0.1; total time=  17.1s\n",
      "[CV] END .....svc__C=10, svc__max_iter=100000, svc__tol=0.01; total time=  20.6s\n",
      "[CV] END ......svc__C=10, svc__max_iter=500000, svc__tol=0.1; total time=  17.9s\n",
      "[CV] END ......svc__C=1, svc__max_iter=100000, svc__tol=0.01; total time=  28.6s\n",
      "[CV] END ......svc__C=5, svc__max_iter=100000, svc__tol=0.01; total time=  17.9s\n",
      "[CV] END ......svc__C=5, svc__max_iter=500000, svc__tol=0.01; total time=  17.5s\n",
      "[CV] END ......svc__C=10, svc__max_iter=100000, svc__tol=0.1; total time=  19.7s\n",
      "[CV] END ......svc__C=10, svc__max_iter=500000, svc__tol=0.1; total time=  18.4s\n",
      "[CV] END ......svc__C=1, svc__max_iter=500000, svc__tol=0.01; total time=  28.3s\n",
      "[CV] END ......svc__C=5, svc__max_iter=100000, svc__tol=0.01; total time=  17.2s\n",
      "[CV] END ......svc__C=5, svc__max_iter=500000, svc__tol=0.01; total time=  17.9s\n",
      "[CV] END ......svc__C=10, svc__max_iter=100000, svc__tol=0.1; total time=  19.1s\n",
      "[CV] END .....svc__C=10, svc__max_iter=500000, svc__tol=0.01; total time=  19.1s\n",
      "[CV] END .......svc__C=1, svc__max_iter=100000, svc__tol=0.1; total time=  27.7s\n",
      "[CV] END .......svc__C=1, svc__max_iter=500000, svc__tol=0.1; total time=  17.1s\n",
      "[CV] END .......svc__C=5, svc__max_iter=100000, svc__tol=0.1; total time=  18.0s\n",
      "[CV] END .....svc__C=10, svc__max_iter=100000, svc__tol=0.01; total time=  19.8s\n",
      "[CV] END .....svc__C=10, svc__max_iter=500000, svc__tol=0.01; total time=  19.7s\n",
      "[CV] END .......svc__C=1, svc__max_iter=100000, svc__tol=0.1; total time=  28.1s\n",
      "[CV] END ......svc__C=5, svc__max_iter=100000, svc__tol=0.01; total time=  17.7s\n",
      "[CV] END ......svc__C=5, svc__max_iter=500000, svc__tol=0.01; total time=  17.8s\n",
      "[CV] END ......svc__C=10, svc__max_iter=100000, svc__tol=0.1; total time=  21.3s\n",
      "[CV] END ......svc__C=10, svc__max_iter=500000, svc__tol=0.1; total time=  17.2s\n",
      "[CV] END ......svc__C=1, svc__max_iter=100000, svc__tol=0.01; total time=  28.3s\n",
      "[CV] END .......svc__C=1, svc__max_iter=500000, svc__tol=0.1; total time=  17.5s\n",
      "[CV] END ......svc__C=5, svc__max_iter=500000, svc__tol=0.01; total time=  17.6s\n",
      "[CV] END .....svc__C=10, svc__max_iter=100000, svc__tol=0.01; total time=  21.7s\n",
      "[CV] END ......svc__C=10, svc__max_iter=500000, svc__tol=0.1; total time=  17.6s\n",
      "[CV] END ......svc__C=1, svc__max_iter=500000, svc__tol=0.01; total time=  27.4s\n",
      "[CV] END .......svc__C=1, svc__max_iter=500000, svc__tol=0.1; total time=  17.3s\n",
      "[CV] END ......svc__C=5, svc__max_iter=500000, svc__tol=0.01; total time=  17.5s\n",
      "[CV] END .....svc__C=10, svc__max_iter=100000, svc__tol=0.01; total time=  22.8s\n",
      "[CV] END ......svc__C=10, svc__max_iter=500000, svc__tol=0.1; total time=  17.4s\n"
     ]
    }
   ],
   "source": [
    "print(\"%s - This review is %s\" % (Xnew, ynew[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408b2f66-fdbe-4f63-bb42-2962e3c4d7f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
